{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/4_xtd3hj54s46hcqc_qgl8dm0000gn/T/ipykernel_85589/2382939539.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds = pd.read_csv('lenta-ru-news.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>tags</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
       "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
       "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
       "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
       "      <td>Министерство народного просвещения, в виду про...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
       "      <td>1914. Das ist Nesteroff!</td>\n",
       "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
       "      <td>1914. Бульдог-гонец под Льежем</td>\n",
       "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
       "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
       "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url  \\\n",
       "0   https://lenta.ru/news/1914/09/16/hungarnn/   \n",
       "1  https://lenta.ru/news/1914/09/16/lermontov/   \n",
       "2  https://lenta.ru/news/1914/09/17/nesteroff/   \n",
       "3   https://lenta.ru/news/1914/09/17/bulldogn/   \n",
       "4       https://lenta.ru/news/1914/09/18/zver/   \n",
       "\n",
       "                                               title  \\\n",
       "0  1914. Русские войска вступили в пределы Венгрии     \n",
       "1  1914. Празднование столетия М.Ю. Лермонтова от...   \n",
       "2                           1914. Das ist Nesteroff!   \n",
       "3                    1914. Бульдог-гонец под Льежем    \n",
       "4           1914. Под Люблином пойман швабский зверь   \n",
       "\n",
       "                                                text       topic  \\\n",
       "0  Бои у Сопоцкина и Друскеник закончились отступ...  Библиотека   \n",
       "1  Министерство народного просвещения, в виду про...  Библиотека   \n",
       "2  Штабс-капитан П. Н. Нестеров на днях, увидев в...  Библиотека   \n",
       "3  Фотограф-корреспондент Daily Mirror рассказыва...  Библиотека   \n",
       "4  Лица, приехавшие в Варшаву из Люблина, передаю...  Библиотека   \n",
       "\n",
       "             tags        date  \n",
       "0  Первая мировая  1914/09/16  \n",
       "1  Первая мировая  1914/09/16  \n",
       "2  Первая мировая  1914/09/17  \n",
       "3  Первая мировая  1914/09/17  \n",
       "4  Первая мировая  1914/09/18  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('lenta-ru-news.csv')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800975, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url          0\n",
       "title        0\n",
       "text         5\n",
       "topic    62002\n",
       "tags     27219\n",
       "date         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712654, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dropna(inplace= True)\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample = ds.sample(n=1000)\n",
    "ds_sample.to_csv('ds_sample.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('ds_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.dropna(inplace = True)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Все': 626,\n",
       "         'Политика': 50,\n",
       "         'Общество': 36,\n",
       "         'Происшествия': 30,\n",
       "         'Украина': 29,\n",
       "         'Госэкономика': 18,\n",
       "         'Футбол': 17,\n",
       "         'Бизнес': 17,\n",
       "         'Интернет': 12,\n",
       "         'Наука': 11,\n",
       "         'Летние виды': 10,\n",
       "         'Следствие и суд': 9,\n",
       "         'Музыка': 9,\n",
       "         'Кино': 7,\n",
       "         'Стиль': 7,\n",
       "         'Coцсети': 6,\n",
       "         'Космос': 6,\n",
       "         'Люди': 6,\n",
       "         'Преступность': 5,\n",
       "         'Искусство': 5,\n",
       "         'Регионы': 5,\n",
       "         'Игры': 5,\n",
       "         'Город': 5,\n",
       "         'Явления': 4,\n",
       "         'Прибалтика': 4,\n",
       "         'Гаджеты': 4,\n",
       "         'Бокс и ММА': 4,\n",
       "         'Конфликты': 3,\n",
       "         'События': 3,\n",
       "         'Квартира': 3,\n",
       "         'Москва': 3,\n",
       "         'Деловой климат': 3,\n",
       "         'Криминал': 3,\n",
       "         'ТВ и радио': 3,\n",
       "         'Закавказье': 3,\n",
       "         'Мир': 2,\n",
       "         'Рынки': 2,\n",
       "         'Пресса': 2,\n",
       "         'Оружие': 2,\n",
       "         'Дача': 2,\n",
       "         'Преступная Россия': 2,\n",
       "         'Техника': 2,\n",
       "         'Мировой бизнес': 2,\n",
       "         'Средняя Азия': 1,\n",
       "         'Театр': 1,\n",
       "         'Офис': 1,\n",
       "         'Деньги': 1,\n",
       "         'Вирусные ролики': 1,\n",
       "         'Мнения': 1,\n",
       "         'Молдавия': 1,\n",
       "         'Звери': 1,\n",
       "         'Белоруссия': 1,\n",
       "         'Зимние виды': 1,\n",
       "         'Внешний вид': 1,\n",
       "         'Киберпреступность': 1,\n",
       "         'Книги': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ds_sample.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Все': 453581,\n",
       " 'Политика': 33220,\n",
       " 'Общество': 27560,\n",
       " 'Украина': 17934,\n",
       " 'Происшествия': 15972,\n",
       " 'Футбол': 13128,\n",
       " 'Госэкономика': 12869,\n",
       " 'Кино': 9109,\n",
       " 'Бизнес': 8224,\n",
       " 'Интернет': 7945,\n",
       " 'Наука': 6975,\n",
       " 'Следствие и суд': 6825,\n",
       " 'Музыка': 5822,\n",
       " 'Люди': 5184,\n",
       " 'Преступность': 4737,\n",
       " 'Квартира': 3759,\n",
       " 'Космос': 3622,\n",
       " 'События': 3161,\n",
       " 'Конфликты': 3136,\n",
       " 'ТВ и радио': 3028,\n",
       " 'Coцсети': 2924,\n",
       " 'Летние виды': 2893,\n",
       " 'Деловой климат': 2656,\n",
       " 'Регионы': 2639,\n",
       " 'Криминал': 2550,\n",
       " 'Явления': 2523,\n",
       " 'Бокс и ММА': 2443,\n",
       " 'Звери': 2278,\n",
       " 'Город': 2247,\n",
       " 'Гаджеты': 2183,\n",
       " 'Мир': 2121,\n",
       " 'Стиль': 2090,\n",
       " 'Игры': 2072,\n",
       " 'Рынки': 2012,\n",
       " 'Пресса': 1805,\n",
       " 'Искусство': 1795,\n",
       " 'Зимние виды': 1677,\n",
       " 'Полиция и спецслужбы': 1635,\n",
       " 'Закавказье': 1442,\n",
       " 'Деньги': 1361,\n",
       " 'Москва': 1312,\n",
       " 'Прибалтика': 1242,\n",
       " 'Театр': 1217,\n",
       " 'Оружие': 1216,\n",
       " 'Техника': 1208,\n",
       " 'Книги': 1208,\n",
       " 'Средняя Азия': 1131,\n",
       " 'Дача': 1084,\n",
       " 'Белоруссия': 916,\n",
       " 'Хоккей': 915,\n",
       " 'Мировой бизнес': 854,\n",
       " 'Движение': 834,\n",
       " 'Офис': 783,\n",
       " 'Внешний вид': 727,\n",
       " 'Достижения': 718,\n",
       " 'Инструменты': 710,\n",
       " 'Россия': 706,\n",
       " 'Часы': 662,\n",
       " 'Софт': 652,\n",
       " 'Мнения': 615,\n",
       " 'Мемы': 477,\n",
       " 'Вирусные ролики': 474,\n",
       " 'Еда': 457,\n",
       " 'Молдавия': 439,\n",
       " 'Вещи': 424,\n",
       " 'Реклама': 389,\n",
       " 'История': 347,\n",
       " 'Жизнь': 333,\n",
       " 'Автобизнес': 310,\n",
       " 'Киберпреступность': 169,\n",
       " 'Финансы компаний': 155,\n",
       " 'Преступная Россия': 144,\n",
       " 'Авто': 141,\n",
       " 'Туризм': 85,\n",
       " 'Социальная сфера': 70,\n",
       " 'Первая мировая': 65,\n",
       " 'Вкусы': 54,\n",
       " 'Экология': 53,\n",
       " 'Наследие': 46,\n",
       " 'Госрегулирование': 41,\n",
       " 'Производители': 36,\n",
       " 'Аналитика рынка': 28,\n",
       " 'Фотография': 28,\n",
       " 'Страноведение': 21,\n",
       " 'Выборы': 11,\n",
       " 'Мировой опыт': 6,\n",
       " 'Вооружение': 3,\n",
       " 'Инновации': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntr = Counter(ds.tags)\n",
    "{k: v for k, v in sorted(cntr.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\,'\n",
      "/var/folders/_3/4_xtd3hj54s46hcqc_qgl8dm0000gn/T/ipykernel_85589/2946871861.py:11: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'''\n",
      "/var/folders/_3/4_xtd3hj54s46hcqc_qgl8dm0000gn/T/ipykernel_85589/2946871861.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  no_punc_text = [rem_punc(t) for t in tqdm_notebook(texts)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f965d760c94488fb535bd6030159479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = ds_sample.text.values\n",
    "#remove punctuation\n",
    "\n",
    "def rem_punc(t):\n",
    "# define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'''\n",
    "\n",
    "\n",
    "# remove punctuation from the string\n",
    "    for c in punctuations:\n",
    "        t=t.replace(c,' ')\n",
    "\n",
    "\n",
    "# display the unpunctuated string\n",
    "    return t.replace('\\n',' ')\n",
    "\n",
    "\n",
    "no_punc_text = [rem_punc(t) for t in tqdm_notebook(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/4_xtd3hj54s46hcqc_qgl8dm0000gn/T/ipykernel_85589/1263936609.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  lc_text = [t.lower() for t in tqdm_notebook(no_punc_text)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d111623f1d4978a462b51ad62f15a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'питерский  зенит  сделал предложение об условиях личного контракта полузащитнику московского  динамо  и сборной россии по футболу игорю семшову  об этом пишет газета   спорт экспресс   если игрок примет предложение  его зарплата вырастет почти в два раза  в составе  зенита  семшов может заменить на позиции центрального полузащитника нынешнего капитана команды анатолия тимощука  который недавно изъявил желание покинуть питерский клуб  в контракте    летнего футболиста с  динамо  прописана сумма отступных  по данным издания  она составляет около восьми миллионов евро   при выплате которых руководство московского клуба не сможет препятствовать переходу игрока  ранее интерес к семшову проявляли чемпион россии казанский  рубин   а также московский  спартак   сам футболист признал  что у него есть предложения  но отметил  что вначале побеседует с руководством  динамо   срок действующего соглашения семшова с  динамо  истекает в конце      года '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "lc_text = [t.lower() for t in tqdm_notebook(no_punc_text)]\n",
    "lc_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopword removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['Я', 'очень', 'люблю', 'изучать', 'обработку', 'естественного', 'языка']\n",
      "Without stopwords: ['очень', 'люблю', 'изучать', 'обработку', 'естественного', 'языка']\n"
     ]
    }
   ],
   "source": [
    "text = \"Я очень люблю изучать обработку естественного языка\"\n",
    "\n",
    "# tokenize into words\n",
    "words = nltk.word_tokenize(text, language=\"russian\")\n",
    "\n",
    "# filter out stopwords\n",
    "filtered = [w for w in words if w.lower() not in sw]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"Without stopwords:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>tags</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
       "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
       "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
       "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
       "      <td>Министерство народного просвещения, в виду про...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
       "      <td>1914. Das ist Nesteroff!</td>\n",
       "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
       "      <td>1914. Бульдог-гонец под Льежем</td>\n",
       "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
       "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
       "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
       "      <td>Библиотека</td>\n",
       "      <td>Первая мировая</td>\n",
       "      <td>1914/09/18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url  \\\n",
       "0   https://lenta.ru/news/1914/09/16/hungarnn/   \n",
       "1  https://lenta.ru/news/1914/09/16/lermontov/   \n",
       "2  https://lenta.ru/news/1914/09/17/nesteroff/   \n",
       "3   https://lenta.ru/news/1914/09/17/bulldogn/   \n",
       "4       https://lenta.ru/news/1914/09/18/zver/   \n",
       "\n",
       "                                               title  \\\n",
       "0  1914. Русские войска вступили в пределы Венгрии     \n",
       "1  1914. Празднование столетия М.Ю. Лермонтова от...   \n",
       "2                           1914. Das ist Nesteroff!   \n",
       "3                    1914. Бульдог-гонец под Льежем    \n",
       "4           1914. Под Люблином пойман швабский зверь   \n",
       "\n",
       "                                                text       topic  \\\n",
       "0  Бои у Сопоцкина и Друскеник закончились отступ...  Библиотека   \n",
       "1  Министерство народного просвещения, в виду про...  Библиотека   \n",
       "2  Штабс-капитан П. Н. Нестеров на днях, увидев в...  Библиотека   \n",
       "3  Фотограф-корреспондент Daily Mirror рассказыва...  Библиотека   \n",
       "4  Лица, приехавшие в Варшаву из Люблина, передаю...  Библиотека   \n",
       "\n",
       "             tags        date  \n",
       "0  Первая мировая  1914/09/16  \n",
       "1  Первая мировая  1914/09/16  \n",
       "2  Первая мировая  1914/09/17  \n",
       "3  Первая мировая  1914/09/17  \n",
       "4  Первая мировая  1914/09/18  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1914. Русские войска вступили в\\xa0пределы Венгрии  '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(ds['title'][0], language=\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1914', '.', 'Русские', 'войска', 'вступили', 'в', 'пределы', 'Венгрии']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data set from stop words\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

import nltk
from nltk.corpus import reuters
from nltk import bigrams, trigrams
from collections import Counter, defaultdict
from tqdm import tqdm_notebook

nltk.download('reuters')

# Create a placeholder for model
model = defaultdict(lambda: defaultdict(lambda: 0))

# Count frequency of co-occurance
for sentence in reuters.sents():
    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):
        model[(w1, w2)][w3] += 1

# Let's transform the counts to probabilities
for w1_w2 in model:
    total_count = float(sum(model[w1_w2].values()))
for w3 in model[w1_w2]:
    model[w1_w2][w3] /= total_count


import random

# starting words
text = ["tariffs", "on"]
sentence_finished = False

while not sentence_finished:
# select a random probability threshold
    r = random.random()
    accumulator = .0



for word in model[tuple(text[-2:])].keys():
    accumulator += model[tuple(text[-2:])][word]
# select words that are above the probability threshold
    if accumulator >= r:
        text.append(word)
        break



    if text[-2:] == [None, None]:
        sentence_finished = True



print(' '.join([t for t in text if t]))

from keras.utils import to_categorical
import numpy as np
import pandas as pd
from keras.utils import to_categorical
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense, GRU, Embedding
from keras.callbacks import EarlyStopping, ModelCheckpoint

data_text = """The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation.
We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a candid world.
He has refused his Assent to Laws, the most wholesome and necessary for the public good.
He has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.
He has refused to pass other Laws for the accommodation of large districts of people, unless those people would relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.
He has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of their public Records, for the sole purpose of fatiguing them into compliance with his measures.
He has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights of the people.
He has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining in the mean time exposed to all the dangers of invasion from without, and convulsions within.
He has endeavoured to prevent the population of these States; for that purpose obstructing the Laws for Naturalization of Foreigners; refusing to pass others to encourage their migrations hither, and raising the conditions of new Appropriations of Lands.
He has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary powers.
He has made Judges dependent on his Will alone, for the tenure of their offices, and the amount and payment of their salaries.
He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass our people, and eat out their substance.
He has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.
He has affected to render the Military independent of and superior to the Civil power.
He has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged by our laws; giving his Assent to their Acts of pretended Legislation:
For Quartering large bodies of armed troops among us:
For protecting them, by a mock Trial, from punishment for any Murders which they should commit on the Inhabitants of these States:
For cutting off our Trade with all parts of the world:
For imposing Taxes on us without our Consent:
For depriving us in many cases, of the benefits of Trial by Jury:
For transporting us beyond Seas to be tried for pretended offences
For abolishing the free System of English Laws in a neighbouring Province, establishing therein an Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit instrument for introducing the same absolute rule into these Colonies:
For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms of our Governments:
For suspending our own Legislatures, and declaring themselves invested with power to legislate for us in all cases whatsoever.
He has abdicated Government here, by declaring us out of his Protection and waging War against us.
He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.
He is at this time transporting large Armies of foreign Mercenaries to compleat the works of death, desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled in the most barbarous ages, and totally unworthy the Head of a civilized nation.
He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.
He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction of all ages, sexes and conditions.
In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury. A Prince whose character is thus marked by every act which may define a Tyrant, is unfit to be the ruler of a free people.
Nor have We been wanting in attentions to our Brittish brethren. We have warned them from time to time of attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, and we have conjured them by the ties of our common kindred to disavow these usurpations, which, would inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.
We, therefore, the Representatives of the united States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all Allegiance to the British Crown, and that all political connection between them and the State of Great Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and Things which Independent States may of right do. And for the support of this Declaration, with a firm reliance on the protection of divine Providence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor."""

import re

def text_cleaner(text):
# lower case text
    newString = text.lower()
    newString = re.sub(r"'s\b","",newString)
# remove punctuations
    newString = re.sub("[^a-zA-Z]", " ", newString)
    long_words=[]
# remove short word
    for i in newString.split():
        if len(i)>=3:
            long_words.append(i)
    return (" ".join(long_words)).strip()



# preprocess the text
data_new = text_cleaner(data_text)
def create_seq(text):
    length = 30
    sequences = list()
    for i in range(length, len(text)):
# select sequence of tokens
        seq = text[i-length:i+1]
# store
    sequences.append(seq)
    print('Total Sequences: %d' % len(sequences))
    return sequences

# create sequences
sequences = create_seq(data_new)


import tensorflow
tensorflow.__version__
# create a character mapping index
chars = sorted(list(set(data_new)))
mapping = dict((c, i) for i, c in enumerate(chars))
def encode_seq(seq):
    sequences = list()
    for line in seq:
# integer encode line
        encoded_seq = [mapping[char] for char in line]
# store
        sequences.append(encoded_seq)
    return sequences
# encode the sequences
sequences = encode_seq(sequences)


from sklearn.model_selection import train_test_split

# vocabulary size
vocab = len(mapping)
sequences = np.array(sequences)
# create X and y
X, y = sequences[:,:-1], sequences[:,-1]
# one hot encode y
y = to_categorical(y, num_classes=vocab)
# create train and validation sets
X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)

# define model
model = Sequential()
model.add(Embedding(vocab, 50, input_length=30, trainable=True))
model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))
model.add(Dense(vocab, activation='softmax'))
print(model.summary())

# compile the model
model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')
# fit the model
model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))

    # Import required libraries
import torch
#from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers import GPT2LMHeadModel, GPT2Tokenizer
    # Load pre-trained model tokenizer (vocabulary)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# Encode a text inputs
text = "What is the fastest car in the"
indexed_tokens = tokenizer.encode(text)

# Load pre-trained model (weights)
model = GPT2LMHeadModel.from_pretrained('gpt2')
# Set the model in evaluation mode to deactivate the DropOut modules
model.eval()

# Predict all tokens
with torch.no_grad():
outputs = model(tokens_tensor)
predictions = outputs[0]

# Get the predicted next sub-word
predicted_index = torch.argmax(predictions[0, -1, :]).item()
predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])
    # Print the predicted word
print(predicted_text)
​
    # Convert indexed tokens in a PyTorch tensor
tokens_tensor = torch.tensor([indexed_tokens])

poem='''
Two households, both alike in dignity,
In fair Verona, where we lay our scene,
From ancient grudge break to new mutiny,
Where civil blood makes civil hands unclean.
From forth the fatal loins of these two foes'''

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
input_ids = tokenizer.encode(poem, add_special_tokens=False, return_tensors="pt")

output_sequences = model.generate(
input_ids=input_ids,
max_length=100 + len(input_ids[0]),
temperature=1.0,
top_k=0,
top_p=0.9,
repetition_penalty=1.0,
do_sample=True,
num_return_sequences=5,
)
#Write your own algorithms for Lemmatization#
def lemmatize(word):
    word = word.lower()
    
    # irregular dictionary
    irregulars = {
        "mice": "mouse",
        "children": "child",
        "geese": "goose",
        "went": "go",
        "done": "do",
        "better": "good",
        "worse": "bad"
    }
    if word in irregulars:
        return irregulars[word]
    
    # plural rules
    if word.endswith("ies") and len(word) > 3:
        return word[:-3] + "y"
    elif word.endswith("es") and len(word) > 2:
        return word[:-2]
    elif word.endswith("s") and len(word) > 1:
        return word[:-1]
    
    # verb forms
    if word.endswith("ing") and len(word) > 4:
        base = word[:-3]
        if base.endswith(base[-1] * 2):  # double consonant rule: running → run
            base = base[:-1]
        elif base.endswith("k"):  # like "making" → "make"
            base += "e"
        return base
    if word.endswith("ed") and len(word) > 3:
        base = word[:-2]
        if base.endswith("i"):  # studied → study
            base = base[:-1] + "y"
        elif base.endswith(base[-1] * 2):  # stopped → stop
            base = base[:-1]
        return base
    
    return word

print(lemmatize("stories"))
print(lemmatize("boxes"))
print(lemmatize("cats"))
print(lemmatize("running"))
print(lemmatize("making"))
print(lemmatize("studied"))
print(lemmatize("children"))
print(lemmatize("went"))

import pandas as pd
import numpy as np
from tqdm import tqdm_notebook
ds = pd.read_csv('lenta-ru-news.csv')
ds.head()
ds.shape
ds.isnull().sum()
ds.dropna(inplace= True)
ds.shape
ds_sample = ds.sample(n=1000)
ds_sample.to_csv('ds_sample.csv',index = False)
new_data = pd.read_csv('ds_sample.csv')
new_data.dropna(inplace = True)
new_data.shape
from collections import Counter
Counter(ds_sample.tags)

cntr = Counter(ds.tags)
{k: v for k, v in sorted(cntr.items(), key=lambda item: item[1], reverse=True)}

texts = ds_sample.text.values
#remove punctuation

def rem_punc(t):
# define punctuation
    punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'''

# remove punctuation from the string
    for c in punctuations:
        t=t.replace(c,' ')

# display the unpunctuated string
    return t.replace('\n',' ')

no_punc_text = [rem_punc(t) for t in tqdm_notebook(texts)]
#lowercase
lc_text = [t.lower() for t in tqdm_notebook(no_punc_text)]
lc_text[0]

#stopword removal
import nltk
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
sw = stopwords.words('russian')

text = "Я очень люблю изучать обработку естественного языка"

# tokenize into words
words = nltk.word_tokenize(text, language="russian")

# filter out stopwords
filtered = [w for w in words if w.lower() not in sw]

print("Original:", words)
print("Without stopwords:", filtered)

ds['title'][0]
words = nltk.word_tokenize(ds['title'][0], language="russian")

# Clean the data set from stop words
# your code
clean_texts = []

for t in tqdm_notebook(lc_text):
    words = nltk.word_tokenize(t, language="russian")   # токенизация
    filtered = [w for w in words if w not in sw]        # убираем стоп-слова
    clean_texts.append(filtered)

# посмотрим первый результат
print(clean_texts[0][:50])
from PIL import Image
import pytesseract
import numpy as np
import os
import matplotlib.pyplot as plt
%matplotlib inline

os.listdir('C:\Program Files (x86)\Tesseract-OCR\tesseract.exe')
os.listdir()
pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe'
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe'
pytesseract.pytesseract.tesseract_cmd = r'tesseract.exe'
filename = 'text_seminar_en.png'
#filename = 'latin_text2.jpg'
img1 = np.array(Image.open(filename))
text = pytesseract.image_to_string(img1)

print(dir(pytesseract.pytesseract.tesseract_cmd))
plt.imshow(img1);

print(text)


import os
import speech_recognition as sr
recognizer = sr.Recognizer()

recognizer.energy_threshold = 500
clean_support_call = sr.AudioFile("Easy Kazakh Conversation - Dialogue.wav")
with clean_support_call as source:
    clean_support_call_audio = recognizer.record(source)

# Transcribe AudioData to text
text = recognizer.recognize_google(clean_support_call_audio,
                                   language="kk-KZ")
# text = recognizer.recognize_google(clean_support_call_audio,
#                                    language="ru-RU")
    
print(text)

# Importing the speech_recognition library
import speech_recognition as sr
recognizer = sr.Recognizer()
# Convert audio to AudioFile
noisy_support_call = sr.AudioFile("Easy Kazakh Conversation - Dialogue.wav")
# Record the audio from the noisy support call
with noisy_support_call as source:
# Adjust the recognizer energy threshold for ambient noise
    recognizer.adjust_for_ambient_noise(source, duration=0.5)
    noisy_support_call_audio = recognizer.record(noisy_support_call)

#Transcribe the speech from the noisy support call
text = recognizer.recognize_google(noisy_support_call_audio,
                                   language="kk-KZ")
print(text)

import selenium
from selenium import webdriver
#from pyvirtualdisplay import Display
from selenium.webdriver.common.keys import Keys
#Chrome
options = webdriver.ChromeOptions()
#options.add_argument('headless')
#options.add_argument('--headless')
prefs = {"profile.default_content_setting_values.notifications" : 2}
options.add_experimental_option("prefs",prefs)
options.add_argument('--disable-logging')
options.add_argument('--log-level=3')
driver = webdriver.Chrome(options=options)
#driver = webdriver.Chrome()
print('Driver started successfully!')
os.getcwd()
driver.get("https://americanliterature.com/childrens-stories/the-three-little-pigs")
driver.get("https://zakup.sk.kz/#/ext")
driver.get("https://egov.kz/cms/ru")

page_text=driver.find_elements_by_tag_name("body")[0].text

from selenium.webdriver.common.by import By

page_text=driver.find_elements(By.TAG_NAME, "body")[0].text
page_text.split('\n')
page_text=page_text.split('\n')

sentences=[]
for item in page_text[9:-3]:
    sentences=sentences+item.split('. ')


punctuation='~!@#$%^&*()-_+=:;\'\",./?'
de_punct_sent=[]
for s in sentences:
    for p in punctuation:
        s=s.replace(p,' ')
    s=s.strip()
    de_punct_sent.append(s)

'a'.islower()
unbroken=[]
for i,s in enumerate(de_punct_sent):
    if s[0].islower():
        unbroken[-1]=unbroken[-1]+' '+s
    else:
        unbroken.append(s)
        
de_punct_sent[0].lower().split()
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer 
nltk.download('wordnet')

def get_pos_tag(t):
    try:
        tag_dict = {"J": wordnet.ADJ,
                    "N": wordnet.NOUN,
                    "V": wordnet.VERB,
                    "R": wordnet.ADV,
                    "P": wordnet.NOUN,#
                    "C": wordnet.NOUN,
                    "D": wordnet.NOUN,
                    "E": wordnet.NOUN,
                    "I": wordnet.NOUN,
                    "L": wordnet.NOUN,
                    "M": wordnet.NOUN,
                    "T": wordnet.NOUN,
                    "U": wordnet.NOUN,
                    "W": wordnet.NOUN,
                    "F": wordnet.NOUN,
                    "S": wordnet.NOUN}
        return tag_dict[t[0]]
    except:
        return wordnet.NOUN

os.listdir()
lemmatizer = WordNetLemmatizer()
print(type(lemmatizer))

nltk.download('averaged_perceptron_tagger')
lemmatizer = WordNetLemmatizer()
lemmed_sents=[''.join([lemmatizer.lemmatize(tg[0], pos=get_pos_tag(tg[1])) for tg in nltk.pos_tag(s.split())]) for s in de_punct_sent]


print(de_punct_sent[:5])
print()
print(lemmed_sents[:5])

#SKLearn tfidfvectorizer
#vec without leemm and with
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(de_punct_sent)
print(vectorizer.get_feature_names())
print(X.shape)

vectorizer2 = TfidfVectorizer()
X2 = vectorizer2.fit_transform(lemmed_sents)
print(vectorizer2.get_feature_names())
print(X2.shape)


from sklearn.decomposition import PCA
def get_2d_3d_rep(X):
    #2D
    pca = PCA(n_components=2)
    X_2d=pca.fit_transform(X)
    
    #3D
    pca = PCA(n_components=3)
    X_3d=pca.fit_transform(X)
    
    return X_2d, X_3d

X_2d, X_3d=get_2d_3d_rep(X.toarray())
X2_2d, X2_3d=get_2d_3d_rep(X2.toarray())

import plotly.express as px
fig = px.scatter(x=X_2d[:,0], y=X_2d[:,1], hover_data=[de_punct_sent])
fig.show()

fig = px.scatter_3d(x=X_3d[:,0], y=X_3d[:,1], z=X_3d[:,2], hover_data=[de_punct_sent])
fig.show()

fig = px.scatter(x=X2_2d[:,0], y=X2_2d[:,1], hover_data=[de_punct_sent])
fig.show()
fig = px.scatter_3d(x=X2_3d[:,0], y=X2_3d[:,1], z=X2_3d[:,2], hover_data=[de_punct_sent])
fig.show()
